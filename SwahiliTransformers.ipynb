{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00ed08b5-2fec-4ca9-809e-05212c18b722",
   "metadata": {},
   "source": [
    "SET UP A VIRTUAL ENVIRONMENT -your_env_name- WITH PYTHON 3.10 (conda create -n your_env_name python=3.10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547f64f-6778-4aaf-8560-8a154b68c2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 INSTALL REQUIRED LIBRARIES\n",
    "\n",
    "#!pip install keras==2.15.0\n",
    "#!pip install tensorboard==2.15.0\n",
    "#!pip install ml-dtypes==0.2.0\n",
    "#!pip install transformers datasets torch sentencepiece\n",
    "#!pip install tensorflow\n",
    "#!pip install accelerate>=0.26.0\n",
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54611809-60e9-4641-b17b-5ff7388ff905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENVIRONMENT VARIABLES THAT I HAD TO SET AFTERWARDS TO IGNORE/AVOID ERRORS WHEN CONFIGURING TRAINING ARGUMENTS STEP 4\n",
    "#(The error is occurring because transformers currently has compatibility issues with Keras 3, which is bundled with recent versions of TensorFlow)\n",
    "\n",
    "#!pip uninstall tensorflow tensorflow-macos keras -y\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "#os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\" #FOR EXTENDING MEMORY ALLOCATION, MIGHT HANG YOUR COMPUTER\n",
    "\n",
    "\n",
    "# MAKE SURE WE HAVE TENSORFLOW INSTALLED AND TAKE NOTE OF CURRENT DIRECTORY\n",
    "#import tensorflow as tf\n",
    "#print(tf.__version__)\n",
    "\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623288c5-b9fd-4bcd-a680-9127ddf74906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 LOAD THE DATA\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Replace 'path/to/swahili_text.txt' with the path to your text file. MINE HAS 34M WORDS, 1.64M LINES, 197M CHARS\n",
    "dataset = load_dataset('text', data_files='swahili2.txt')\n",
    "\n",
    "# Display the first 5 rows of the dataset\n",
    "print(dataset['train'].select(range(5)))\n",
    "\n",
    "# Alternatively, if you just want to print the text field:\n",
    "for i in range(5):\n",
    "    print(dataset['train'][i]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c35b419-d538-4793-807d-c76f6811d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-1 USE A PRE-EXISTING TOKENIZER\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce86e93-d4e0-463d-8c4b-c9f2e23701c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-2 TOKENIZE THE DATASET\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d1df4c-5a87-4259-bcd4-ad1f15f7d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-1 Prepare for Training - CREATE DATA COLLECTOR\n",
    "\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb545b7e-4003-4976-90ac-adb370080b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-2  Prepare for Training - CONFIGURE TRAINING ARGUMENTS\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./swahili_model\",\n",
    "#    eval_strategy=\"epoch\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    ")\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ab091f-f2c6-43ae-aac5-239e31c18b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4-3  Prepare for Training - LOAD MODEL\n",
    "\n",
    "'''from transformers import BertForMaskedLM\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\") # THIS WAS CAUSING AN ERROR MESSAGE\n",
    "\n",
    "'''\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "print(\"Model loaded successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdd2c45-e3f8-4a44-8e41-820f50cc9004",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 TRAIN THE MODEL\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_dataset[\"train\"],\n",
    "   data_collator=data_collator,\n",
    "   tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cc70b4-1cbe-4d78-b551-20dbc459a631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 SAVE THE MODEL\n",
    "\n",
    "model.save_pretrained(\"./swahili_model\")\n",
    "tokenizer.save_pretrained(\"./swahili_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f006bed6-cdb6-4d70-8709-319c919e66fa",
   "metadata": {},
   "source": [
    "USING THE MODEL WEIGHTS\n",
    "\n",
    "Step 1: Load the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bb84d0-f018-47f9-98e4-6aa77bae35db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer from the saved directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./swahili_model\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"./swahili_model\")\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aed9c8-2b59-4485-9a6c-263f75e6ce62",
   "metadata": {},
   "source": [
    "Step 2: Generate Text with Masked Language \n",
    "Since BERT-based models are designed for masked token prediction, you can use it to fill in blanks within a sentence. Hereâ€™s how:\n",
    "\n",
    "1.\tDefine a Prompt with Masked Tokens:\n",
    "Create a Swahili sentence with a [MASK] token where you want the model to predict words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08353806-aa8c-48e9-8314-399be29c25ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Wakati [MASK] leo.\"  # Example prompt in Swahili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907092fe-f11e-4f89-b16b-cc631c1068cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Tokenize input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216fdccb-00eb-428b-849b-26cb91474e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 Predict masked token\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use the fill-mask pipeline for masked token prediction\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Generate predictions\n",
    "result = fill_mask(input_text)\n",
    "for prediction in result:\n",
    "    print(prediction[\"sequence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0388bf-4b52-41e7-bd10-fe1cf0b05297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
